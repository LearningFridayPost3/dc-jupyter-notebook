{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &#127794; Digital Champion - Hands-On Python Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#128210; Inhaltsverzeichnis:\n",
    "* [1 Introduction](#first-bullet)\n",
    "* [2 Decision Trees Theory & Task Description](#second-bullet)\n",
    "* [3 Data Preparation](#third-bullet)\n",
    "* [4 Simple Decision Tree](#fourth-bullet)\n",
    "* [5 Optimized Decision Tree](#fifth-bullet)\n",
    "* [6 Conclusion](#sixth-bullet)\n",
    "* [7 Appendix](#seventh-bullet)\n",
    "* [8 Solutions](#eight-bullet)\n",
    "* [9 Sources](#nineth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction (5 min) <a class=\"anchor\" name=\"first-bullet\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebooks are interactive development environments that allow for the combination of code, visualizations, and text in a single environment. They are frequently used for data analysis, code sharing, and documentation. With Jupyter Notebooks, Python code cells can be executed and the results displayed immediately. In this Jupyter Notebook, we will look at ```Decision Trees```.\n",
    "\n",
    "![alt text for screen readers](./pictures/jupyter_intro_google.png \"Introduction to Jupyter Notebook\").\n",
    "\n",
    "### Explanations:\n",
    "* &#10133; **Code:** With this symbol, we can insert a new code cell into the notebook. Code cells are used to enter executable code. Programming languages such as Python, R, Julia, and others can be used in a code cell.\n",
    "* &#10133; **Text:** With this symbol, we can insert a new text cell into the notebook. Text cells are used for entering and formatting text. They allow for the insertion of text, headings, lists, images, and other formatting elements.\n",
    "* üóëÔ∏è **Delete:** With this symbol, we can delete a cell and remove its content.\n",
    "* &#9654; **Run:** With this symbol, we can execute the code in a cell..\n",
    "* ```Runtime``` -> ```Run All``` The kernel executes the code and returns the results to the notebook. It is possible that the kernel is no longer connected. In this case, we need to restart the kernel and run all cells again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: These two lines must be executed without fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads our dataset from the LearningFriday GitHub repository\n",
    "!git clone https://github.com/LearningFridayPost3/dc-jupyter-notebook.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes the working directory (folder where we search for data) so that data can be read.\n",
    "%cd dc-jupyter-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "&#128712; **INFO:**</b> \n",
    "\n",
    "- We read through the notebook and complete the tasks directly in this notebook.\n",
    "    \n",
    "- We can access the solutions using the link provided under the tasks.\n",
    "    \n",
    "- Please write questions directly in the Teams chat.\n",
    "\n",
    "- More complex questions will be addressed in breakout sessions.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Decision Trees: Theory & Task Description (8 min) <a class=\"anchor\" name=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What is a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a method for automatically classifying data objects (e.g., people or objects such as packages). A decision tree always consists of a root node and any number of internal nodes (split nodes) as well as at least two leaves (leaf nodes). Each node represents a logical rule, and each leaf represents a class. Below is an example of a decision tree:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text for screen readers](./pictures/dt-example-new.png \"Beispiel Entscheidungsbaum\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset with many individuals. For each person, we know their income and whether they have a mortgage. The target variable is whether a particular person has insurance (class: ```Has Insurance```) or not (class: ```No Insurance```). This means that with the depicted decision tree, we want to determine whether a person has insurance using the information from ```income_usd``` and ```with_mortgage```.\n",
    "\n",
    "The following information is included in the decision tree (figure above):\n",
    "\n",
    "* **gini:** The Gini index describes how well a node separates different classes (e.g., ```No Insurance```, ```Has Insurance```). The value is always between 0 and 1. The smaller the Gini index, the better. When constructing the decision tree, the Gini index is calculated for each node. The logical rule with the smallest Gini index is always chosen.\n",
    "* **samples:** This value describes the number of observations (e.g., data from individuals) available for splitting a specific node. For example, we see that data from 24 individuals were used to construct this tree. Furthermore, we see that the first node splits the 24 individuals into one group of 13 and another group of 11.\n",
    "* **value:** Value describes how the ```samples``` are distributed in the node. The value ```[15, 9]``` in the first node, for example, indicates that out of 24 individuals, 15 do not have insurance (class: ```No Insurance```) and 9 have insurance (class: ```Has Insurance```).\n",
    "* **class:** This value shows the class assigned to a specific node. Example: In the first node, we see the class ```No Insurance``` because more of the 24 individuals have No Insurance (15) than ```Has Insurance``` (9). We can also infer the class from the colors. The redder a node is, the more likely it belongs to the class ```No Insurance```, and the bluer a node is, the more likely it belongs to the class ```Has Insurance```.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "&#128712; **INFO:**</b>\n",
    "How to read a decision tree and how the decision tree classifies new observations (e.g., individuals):\n",
    "\n",
    "- We always start at the root node, i.e., at the very top of the decision tree.\n",
    "\n",
    "- If the logical rule in the node is satisfied for the new observation, the left path in the decision tree is chosen. If the logical rule in the node is not satisfied, the right path is chosen.\n",
    "\n",
    "- We traverse the decision tree until we reach a leaf. The ```class``` attribute in the leaf describes the class of the new data object.**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='aufgabe_1'></a>\n",
    "&#9989; **Task 1:**</b> Determine the class of the following two individuals according to the decision tree depicted above:\n",
    "\n",
    "- Person 1: income_usd = 100'000; with_mortgage = 0\n",
    "\n",
    "- Person 2: income_usd = 73'000; with_mortgage = 1\n",
    "\n",
    "-> Solutions to [Task 1](#l√∂sung_aufgabe_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own solution (with # you can write comments that are not interpreted by Python):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Task Description\n",
    "\n",
    "Now we are working with the ['heart-disease'](https://archive.ics.uci.edu/dataset/45/heart+disease) (HD) dataset. This patient data is organized in a table with 14 columns (features) and 303 rows (observations). A brief description of the various features:\n",
    "* age: Age in years\n",
    "* sex: Male/Female\n",
    "* restbp: Resting blood pressure in mm/Hg at hospital admission\n",
    "* chol: Serum cholesterol in mg/dl\n",
    "* fbs: If fasting blood sugar > 120 mg/dl\n",
    "* thalach: Maximum heart rate achieved\n",
    "* exang: Exercise-induced angina (True/False)\n",
    "* oldpeak: ST depression induced by exercise relative to rest\n",
    "* ca: Number of major vessels (0-3) colored by fluoroscopy\n",
    "\n",
    "Our goal is to generate a model with the 303 observations that can classify new observations (or individuals) and thus determine whether heart disease (HD) is present or not. In the model, we use the features described above to predict the target variable (HD):\n",
    "* hd: Type of heart disease (here: binary)\n",
    "\n",
    "In a group of expert data scientists, we discussed which model we wanted to use. We decided to use decision trees for classifying heart diseases because they are easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data preparation (12 min) <a class=\"anchor\" name=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read data\n",
    "\n",
    "For each new Python project, we consider which Python libraries we want to use. A Python library is a reusable block of code that we can integrate into a program or project. Integrating such code blocks is much faster and more reliable than writing the code ourselves.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "&#128712; **INFO:**</b> \n",
    "In Python, anything that follows a '#' is a comment used to describe the code. </div>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pandas numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "# pandas: Library for reading and manipulating data\n",
    "import pandas as pd\n",
    "# numpy: Library for calculating KPIs\n",
    "import numpy as np\n",
    "# plt: Library for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "# DecisionTreeClassifier: Modeling kit for decision trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# plot_tree: Function for plotting decision trees\n",
    "from sklearn.tree import plot_tree\n",
    "# train_test_split: Function to split test objects into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# cross_val_score: Function for cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# confusion_matrix: Function for calculating the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# ConfusionMatrixDispla: Function for plotting the confusion matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# accuracy_score: Function for calculating accuracy\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "&#128712; **INFO:**</b>   In Python, data is stored in a variable using the '=' operator. For example, if we want to store the number 5 in the variable 'a', we can do it with the following code:\n",
    "<br><br>\n",
    "a = 5 \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(filepath_or_buffer, sep, encoding) allows reading '.csv' data\n",
    "# pd refers to the pandas library, which is used for data manipulation. The \".\" indicates that we are using a method (read_csv) from pd.\n",
    "# The output is stored in the variable df as a DataFrame object.\n",
    "df = pd.read_csv(filepath_or_buffer='data/processed_cleveland_small.csv',\n",
    "                 sep=',',\n",
    "                 encoding='latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we call the variable 'df', it will be displayed\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='aufgabe_2'></a>\n",
    "&#9989; **Task 2:**</b>\n",
    "Store the data in the variable 'df_start'.\n",
    "    \n",
    "-> Solution to [Task 2](#l√∂sung_aufgabe_2)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .head() show what the first five rows of df_start look like\n",
    "df_start.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we write the column name in square brackets, e.g. df_start['ColumnName'], we get the values of that column\n",
    "df_start['ca']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last displayed value is a '?'. We want to check if there are more such 'special' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .unique() function shows all the different elements in the 'ca' column\n",
    "df_start['ca'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value '?' is the only non-numeric value in the 'ca' column. We need to be cautious with such values as they could be outliers or missing values. Therefore, the Data Science team contacts the dataset's author. After some clarifications, we are sure that these are missing data. However, we do not yet know how often such missing data occurs.\n",
    "\n",
    "To see how often this value appears, we filter for the value '?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TableName['ColumnName'] == 'text to check'] can be used to filter a table by a specific column value\n",
    "df_start[df_start['ca'] == '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value '?' occurs very rarely. We decide to remove these observations from the dataset. This means we remove every observation with a question mark. In real applications, more sophisticated methods are often used that do not ignore the information in the other variables of such observations. Trees, in particular, can handle this very well automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "&#128712; **INFO:**</b>  The most important comparison operators:\n",
    "    \n",
    "* ```==```: the element to be compared must have the same content\n",
    "* ```!=```: the element to be compared must not have the same content\n",
    "* ```>=```: the number to be compared must be equal to or greater\n",
    "* ```<=```: the number to be compared must be equal to or smaller\n",
    "     \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='aufgabe_3'></a>\n",
    "&#9989; **Task 3:**</b>\n",
    "We have seen that only 4 test objects in the 'ca' column contain a '?'. Since the clarifications have shown that these are missing data, and this phenomenon affects only very few test objects, we want to filter out these values from our 'df_start' table and save the new table as 'df_no_missing'.\n",
    "\n",
    "-> Solution to [Task 3](#l√∂sung_aufgabe_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: With this code, we check if all '?' have been removed.\n",
    "df_no_missing['ca'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Outliers\n",
    "\n",
    "The authors of the dataset mentioned that there are inexplicable values for age. This means we suspect outliers in the following feature:\n",
    "* age: age of the test objects\n",
    "\n",
    "A common way to check for outliers is through visualizations like boxplots. A boxplot is a graphical representation of statistical distributions that visualizes the median, quartiles, and outliers by displaying the data in boxes.\n",
    "\n",
    "![alt text for screen readers](./pictures/boxplot-new.png \"Example Boxplot\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .boxplot(ColumnName) function displays a boxplot for a specific feature\n",
    "df_no_missing.boxplot('age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are no outliers in the ```age``` feature. The authors of the dataset seem to have already cleaned the data. If outliers do occur, they should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data formatting\n",
    "In the next step, the data is split into features and the target variable. All features are included in the DataFrame ```X```, while the values of the target variable are stored in ```y```.\n",
    "\n",
    "Here, ```X``` represents potential observations, i.e., data from individuals, and ```y``` stands for the possible classifications (```0```=```no heart disease```; ```1```=```has heart disease```). The overarching goal is to create a decision tree that classifies ```y``` as accurately as possible based on ```X```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the .copy() function, we create a copy of the table (to avoid altering the original data df_no_missing)\n",
    "df_clean = df_no_missing.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features should be stored in the table 'X'\n",
    "# The .drop('column name', axis=1) function is used to remove a column\n",
    "# With the .copy() function, we create a copy of the table\n",
    "X = df_clean.drop('hd', axis=1).copy()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we store the target variable as 'y'\n",
    "y = df_clean['hd'].copy()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, our data is split into a ```training set``` and a ```test set```:\n",
    "\n",
    "- ```Training set```: This consists of data used to construct a decision tree and train it based on specific observations. The goal is to teach the tree the characteristic properties of a dataset.\n",
    "- ```Test set```: This dataset is used to test the decision tree. It allows for the evaluation of how well the tree can classify new observations and thus assess its accuracy.\n",
    "\n",
    "![alt text for screen readers](./pictures/train_test_split.png \"Picture train test split\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The train_test_split() function splits the data into a training set and a test set\n",
    "# By default, 25% of the data is in the test set and 75% is in the training set\n",
    "# We see that train_test_split(X, y) generates 4 tables (one X and y table for each set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Simple Decision Tree (10 min) <a class=\"anchor\" name=\"fourth-bullet\"></a>\n",
    "### 4.1 Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we construct/train a ```simple decision tree``` (without optimizations) based on our observations in the training set. This decision tree allows for an initial classification of objects (e.g., individuals) regarding heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DecisionTreeClassifier() function is used for constructing a decision tree. Setting the random state ensures reproducible results.\n",
    "clf_dt_e = DecisionTreeClassifier(random_state=42)\n",
    "# .fit(X_train, y_train) assigns a training set (X_train and y_train) to the decision tree\n",
    "clf_dt_e.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualization: With the following code, we can visualize/plot a decision tree\n",
    "# Here, the size of the plot is defined in inches (15 stands for the width and 7.5 stands for the height)\n",
    "plt.figure(figsize=(15, 7.5), dpi=600)\n",
    "\n",
    "# The plot_tree(decision_tree, class_names, feature_names) function is used for visualizing a decision tree\n",
    "# 'decision_tree' represents the decision tree to be visualized\n",
    "plot_tree(decision_tree=clf_dt_e,\n",
    "          # 'filled=True' results in the nodes being filled with colors\n",
    "          filled=True,\n",
    "          # 'class_names=[\"kein HD\", \"hat HD\"]' specifies the classes that should be listed under 'class' in each node\n",
    "          class_names=[\"keine HD\", \"hat HD\"],\n",
    "          # 'feature_names=X.columns' must be provided to ensure the features are correctly labeled in the plot\n",
    "          feature_names=X.columns)\n",
    "\n",
    "# The plt.show() function displays the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! We have constructed our first simple decision tree. Now, we want to see how well this decision tree, which we trained with the training set, can perform classifications on the test set. This means we want to see how new individuals, who were not used for training, are classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .predict(table name) function is used to perform classifications\n",
    "# This function must be provided with a table name as input -> the table must contain the test features\n",
    "predictions = clf_dt_e.predict(X_test)\n",
    "# The output is a list of classifications (0=no HD; 1=has HD)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we want to check whether these classifications are correct or incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confusion_matrix(y_true, y_pred, labels) function provides a way to check the classification\n",
    "# 'y_true' -> list of correct classifications\n",
    "cm = confusion_matrix(y_true=y_test,\n",
    "                      # 'y_pred' -> list of performed classifications\n",
    "                      y_pred=predictions,\n",
    "                      # 'labels' -> provide classes from clf_dt\n",
    "                      labels=clf_dt_e.classes_)\n",
    "\n",
    "# The ConfusionMatrixDisplay(confusion_matrix, display_labels) function is used to visualize the confusion matrix 'cm'\n",
    "# 'confusion_matrix' -> confusion matrix to be visualized\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              # 'display_labels' -> labels, which should be shown in visualization\n",
    "                              display_labels=['keine HD', 'hat HD'])\n",
    "# The .plot() function creates the visualization\n",
    "disp.plot()\n",
    "# The plt.show() function displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphic shown above depicts a ```Confusion Matrix```. It is read as follows:\n",
    "\n",
    "- ```31```: Do not have heart disease -> These 31 were correctly classified (True Negative TN)\n",
    "- ```12```: Do not have heart disease -> These 12 were incorrectly classified (False Negative FN)\n",
    "- ```25```: Have heart disease -> These 25 were correctly classified (True Positive TP)\n",
    "- ```7```: Have heart disease -> These 7 were incorrectly classified (False Positive FP)\n",
    "\n",
    "The effectiveness of a decision tree is defined by its ```Accuracy```, which can be calculated using the ```Confusion Matrix```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "&#128712; **INFO:**</b> \n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "<br>\n",
    "<br>... in our simple decision tree, for example, we have the following accuracy:\n",
    "<br> \n",
    "<br>Accuracy = (25 + 31) / (25 + 31 + 7 + 12) = 0.75\n",
    "    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function accuracy_score(y_true, y_pred) shows the calculated accuracy\n",
    "accuracy_score(y_true=y_test,\n",
    "               y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Optimized decision tree (10 min) <a class=\"anchor\" name=\"fifth-bullet\"></a>\n",
    "### 5.1 Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text for screen readers](./pictures/ccp.png \"Pruning\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have constructed a ```simple decision``` tree so far. Together with the Data Science team, we are discussing whether this decision tree is sufficient. Someone from the team has a doubt: the decision tree might be overfitted to the training set. Overfitting is a known phenomenon. It means that the classification of observations from our training set works very well, but less well for the classification of new observations. The decision tree is therefore too adapted to the training data (overfitted).\n",
    "\n",
    "We can solve this problem by simplifying the construction of the decision tree. We use various parameters (e.g., ```max_depth``` or ```min_samples```) to optimize the decision tree. This results in the decision tree having fewer leaves and thus being simpler and less adapted to the training data. This process is called ```pruning```. The goal is to improve the ```accuracy``` for new observations using pruning.\n",
    "\n",
    "```Cost Complexity Pruning``` is a specific method to find a smaller decision tree that delivers better results for new observations. We look to see if smaller sub-decision trees (tree with 38 leaves, tree with 37 leaves, etc.) deliver better results than larger ones. To compare smaller trees with larger trees, we use the parameter ```alpha``` as a penalty term that improves the result of smaller trees (overfitting vs. accuracy in the test dataset).\n",
    "\n",
    "A clearer manual to ```Cost Complexity Pruning``` can be found [here](#anhang_1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "&#128712; **INFO:**</b> \n",
    "\n",
    "The values of ```alpha``` are to be interpreted as follows:\n",
    "- ``` 0```: The decision tree is not pruned. It has the maximum size and corresponds to the simple decision tree constructed in Chapter 4.\n",
    "- ```>0```: The larger the value of ```alpha```, the simpler the decision tree. This means that as ```alpha``` increases, the decision tree will have fewer leaves.\n",
    "    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .cost_complexity_pruning_path(X_train, y_train) is an algorithm to find the most optimal values for alpha.\n",
    "path = clf_dt_e.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select a value from the list and construct a decision tree with 'alpha' = 0.0077381.\n",
    "value_alpha = 0.0077381\n",
    "\n",
    "# Definition of the decision tree\n",
    "clf_dt_new = DecisionTreeClassifier(random_state=42,\n",
    "                                    ccp_alpha=value_alpha)\n",
    "clf_dt_new.fit(X_train, y_train)\n",
    "\n",
    "# Visualization of the decision tree\n",
    "plot_tree(decision_tree=clf_dt_new,\n",
    "          filled=True,\n",
    "          class_names=[\"keine HD\", \"hat HD\"],\n",
    "          feature_names=X.columns)\n",
    "\n",
    "# The function plt.show() displays the plot.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='aufgabe_4'></a>\n",
    "&#9989; **Task 4:**</b>\n",
    "\n",
    "Construct and visualize three different decision trees with different ```alpha``` values. For the construction, we can use and copy the code above.\n",
    "\n",
    "We should ensure that the decision trees are stored in the following variables (replace ```clf_dt_new``` with ```clf_dt_1```):\n",
    "- clf_dt_1\n",
    "- clf_dt_2\n",
    "- clf_dt_3\n",
    "\n",
    "-> Solution to [Task 4](#l√∂sung_aufgabe_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the constructed decision tree 'clf_dt_new', we want to plot the confusion matrix to check the performance.\n",
    "# Classifications\n",
    "predictions = clf_dt_new.predict(X_test)\n",
    "cm = confusion_matrix(y_true=y_test,\n",
    "                      y_pred=predictions,\n",
    "                      labels=clf_dt_new.classes_)\n",
    "\n",
    "# Visualization of the Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['keine HD', 'hat HD'])\n",
    "\n",
    "# The function plt.show() displays the plot\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='aufgabe_5'></a>\n",
    "&#9989; **Task 5:**</b>\n",
    "\n",
    "Visualize the confusion matrix for the decision trees constructed in Task 4. For the visualizations, we can use and copy the code above.\n",
    "<br>\n",
    "<br>-> Solution to [Task 5](#l√∂sung_aufgabe_5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Own solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that depending on the ```alpha```, the results can get worse or better. Our team is considering how we can easily find the ```alpha``` that yields the best results.\n",
    "\n",
    "Since we don't want to check this manually, we will write a code that does the following for us:\n",
    "1. Construct a decision tree for each ```alpha``` value.\n",
    "2. Calculate the ```accuracy``` for the test set and the training set for each decision tree.\n",
    "3. Visualize the ```accuracy``` for each decision tree as a function of ```alpha```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> <b>Complex code cell:</b> \n",
    "\n",
    "The following code cell only needs to be executed. Understanding its functionality is not part of this introduction.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With [:-1] we remove the largest 'alpha' value (the largest 'alpha' has no leaves).\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "\n",
    "clf_dts = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf_dt = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    clf_dt.fit(X_train, y_train)\n",
    "    clf_dts.append(clf_dt)\n",
    "    \n",
    "# With the code below, we visualize 'accuracy' as a function of 'alpha' (for test data and training data)\n",
    "train_scores = [clf_dt.score(X_train, y_train) for clf_dt in clf_dts]\n",
    "test_scores = [clf_dt.score(X_test, y_test) for clf_dt in clf_dts]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_title('Accuracy vs alpha for training and testing sets')\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label='train', drawstyle='steps-post')\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label='test', drawstyle='steps-post')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this visualization, we find the necessary information we need to determine the best ```alpha```. Since we want to create a decision tree that generalizes well, we are particularly interested in the orange line (```test data```).\n",
    "\n",
    "When selecting the best ```alpha```, we consider the following two points:\n",
    "1. The ```accuracy``` of the ```test``` should be as high as possible.\n",
    "2. The ```accuracy``` of the ```train``` should be as high as possible.\n",
    "\n",
    "Considering these points, the optimal ```alpha``` is the sixth last point on the ```test``` line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the sixth last point, we call up the list of 'alpha' values again\n",
    "ccp_alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define and save the optimal 'alpha'\n",
    "alpha_opt = 0.01081731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Decision Tree\n",
    "clf_dt_pruned = DecisionTreeClassifier(random_state=42,\n",
    "                                       ccp_alpha=alpha_opt)\n",
    "clf_dt_pruned = clf_dt_pruned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the Decision Tree\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "plot_tree(clf_dt_pruned,\n",
    "          filled=True,\n",
    "          class_names=[\"keine HD\", \"hat HD\"],\n",
    "          feature_names=X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Accuracy' of the New Decision Tree\n",
    "predictions_new = clf_dt_pruned.predict(X_test)\n",
    "accuracy_score(y_true=y_test,\n",
    "               y_pred=predictions_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following ```accuracy values```:\n",
    "* simple decision tree = ```0.746``` (75%)\n",
    "* optimized decision tree = ```0.786``` (79%)\n",
    "\n",
    "When we compare these two values, we see that through optimization, we have obtained a decision tree that can handle new observations better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Conclusion (5 min) <a class=\"anchor\" name=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What have we learned:\n",
    "* What are decision trees?\n",
    "* How do we read decision trees?\n",
    "* How do we prepare data (missing data & outliers)?\n",
    "* Working with training and test sets\n",
    "* Creating simple decision trees\n",
    "* Optimizing simple decision trees\n",
    "* Accuracy and confusion matrix\n",
    "\n",
    "What to consider when integrating models:\n",
    "* Generalization\n",
    "* Software engineering skills to embed models in applications\n",
    "* Response times\n",
    "* Monitoring prediction quality\n",
    "* Collaboration of multiple roles (data engineer, data scientist, software engineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Appendix <a class=\"anchor\" name=\"seventh-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='anhang_1'></a>\n",
    "### Appendix 1: Guide to Cost Complexity Pruning\n",
    "\n",
    "The algorithm for cost complexity pruning generally follows these steps:\n",
    "\n",
    "1. Create an initial decision tree with a training dataset considering all available attributes and features.\n",
    "2. Evaluate the classification performance of the decision tree using a separate validation dataset.\n",
    "3. Calculate the potential costs for each internal node of the decision tree in terms of misclassification or other metrics.\n",
    "4. Assign a cost complexity score to each internal node, usually calculated as the sum of its misclassification costs and a penalty term proportional to the number of descending leaf nodes.\n",
    "5. Starting from the root node, iteratively prune the node with the lowest cost complexity score, creating a series of smaller decision trees.\n",
    "6. Evaluate the classification performance of each pruned decision tree using the validation dataset.\n",
    "7. Select the pruned tree with the best performance, often measured by accuracy or another suitable metric.\n",
    "8. Optionally, further prune the selected tree by optimizing the complexity parameter Œ± and repeating steps 5-7.\n",
    "9. The final pruned decision tree is achieved when no further improvements in performance can be obtained through additional pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Solutions <a class=\"anchor\" name=\"eight-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='l√∂sung_aufgabe_1'></a>\n",
    "### Solution Task 1\n",
    "\n",
    "Classes:\n",
    "- Data Object 1 = 'Has Insurance'\n",
    "- Data Object 2 = 'Has Insurance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text for screen readers](./pictures/L√∂sung_mit_Pfaden.png \"L√∂sung\").\n",
    "\n",
    "-> Back to [Task 1](#aufgabe_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='l√∂sung_aufgabe_2'></a>\n",
    "### Solution Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Back to [Task 2](#aufgabe_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='l√∂sung_aufgabe_3'></a>\n",
    "### Solution Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing = df_start[df_start['ca'] != '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Back to [Taks 3](#aufgabe_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='l√∂sung_aufgabe_4'></a>\n",
    "### Solution Task 4\n",
    "\n",
    "The following code cells represent a possible solution to Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7.5), dpi=600)\n",
    "\n",
    "# We select a value from the list and construct a decision tree with 'alpha' = 0.0000000000001\n",
    "value_alpha = 0.0000000000001\n",
    "\n",
    "# Definition of the Decision Tree\n",
    "clf_dt_1 = DecisionTreeClassifier(random_state=42,\n",
    "                                ccp_alpha=value_alpha)\n",
    "clf_dt_1.fit(X_train, y_train)\n",
    "\n",
    "# Visualization of the Decision Tree\n",
    "plot_tree(decision_tree=clf_dt_1,\n",
    "          filled=True,\n",
    "          class_names=[\"keine HD\", \"hat HD\"],\n",
    "          feature_names=X.columns)\n",
    "\n",
    "# The function plt.show() displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7.5), dpi=600)\n",
    "\n",
    "# We select a value from the list and construct a decision tree with 'alpha' = 0.01425422\n",
    "value_alpha = 0.01425422\n",
    "\n",
    "# Definition of the Decision Tree\n",
    "clf_dt_2 = DecisionTreeClassifier(random_state=0,\n",
    "                                ccp_alpha=value_alpha)\n",
    "clf_dt_2.fit(X_train, y_train)\n",
    "\n",
    "# Visualization of the Decision Tree\n",
    "plot_tree(decision_tree=clf_dt_2,\n",
    "          filled=True,\n",
    "          class_names=[\"keine HD\", \"hat HD\"],\n",
    "          feature_names=X.columns)\n",
    "\n",
    "# The function plt.show() displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7.5), dpi=600)\n",
    "\n",
    "# We select a value from the list and construct a decision tree with 'alpha' = 0.2\n",
    "value_alpha = 0.2\n",
    "\n",
    "# Definition of the Decision Tree\n",
    "clf_dt_3 = DecisionTreeClassifier(random_state=42,\n",
    "                                ccp_alpha=value_alpha)\n",
    "clf_dt_3.fit(X_train, y_train)\n",
    "\n",
    "# Visualization of the Decision Tree\n",
    "plot_tree(decision_tree=clf_dt_3,\n",
    "          filled=True,\n",
    "          class_names=[\"keine HD\", \"hat HD\"],\n",
    "          feature_names=X.columns)\n",
    "\n",
    "# The function plt.show() displays the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Back to [Task 4](#aufgabe_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='l√∂sung_aufgabe_5'></a>\n",
    "### Solution Task 5\n",
    "The following code cells represent a possible solution to Task 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifications\n",
    "predictions = clf_dt_1.predict(X_test)\n",
    "cm = confusion_matrix(y_true=y_test,\n",
    "                      y_pred=predictions,\n",
    "                      labels=clf_dt_1.classes_)\n",
    "\n",
    "# Visualization of the Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[\"keine HD\", \"hat HD\"])\n",
    "\n",
    "# The function plt.show() displays the plot\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifications\n",
    "predictions = clf_dt_2.predict(X_test)\n",
    "cm = confusion_matrix(y_true=y_test,\n",
    "                      y_pred=predictions,\n",
    "                      labels=clf_dt_2.classes_)\n",
    "\n",
    "# Visualization of the Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[\"keine HD\", \"hat HD\"])\n",
    "\n",
    "# The function plt.show() displays the plot\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifications\n",
    "predictions = clf_dt_3.predict(X_test)\n",
    "cm = confusion_matrix(y_true=y_test,\n",
    "                      y_pred=predictions,\n",
    "                      labels=clf_dt_3.classes_)\n",
    "\n",
    "# Visualization of the Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[\"keine HD\", \"hat HD\"])\n",
    "\n",
    "# The function plt.show() displays the plot\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Back to [Task 5](#aufgabe_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Sources <a class=\"anchor\" name=\"nineth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &#169; Sources:\n",
    "&#128190; **Data:** https://archive.ics.uci.edu/dataset/45/heart+disease \n",
    "<br>\n",
    "&#128252;  **Video:** https://youtu.be/q90UDEgYqeI?list=PLBq2sVJiEBvA9rPo3IEQsJNI4IJbn81tB\n",
    "\n",
    "### &#128161; Weitere Informationen:\n",
    "\n",
    "``` Decision Trees: ``` &nbsp; https://www.youtube.com/watch?v=7VeUPuFGJHk&t=0s\n",
    "<br>\n",
    "``` Cross Validation: ``` &nbsp; https://www.youtube.com/watch?v=fSytzGwwBVw&t=0s\n",
    "<br>\n",
    "``` Confusion Matrix: ``` &nbsp; https://www.youtube.com/watch?v=Kdsp6soqA7o&t=0s\n",
    "<br>\n",
    "``` Cost-Complexity Pruning: ``` &nbsp; https://www.youtube.com/watch?v=D0efHEJsfHo&t=0s\n",
    "<br>\n",
    "``` Bias and Variance and Overfitting: ``` &nbsp; https://www.youtube.com/watch?v=EuBBz3bI-aA&t=0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
